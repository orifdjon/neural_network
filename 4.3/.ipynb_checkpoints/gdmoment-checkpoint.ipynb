{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = pd.read_csv(filepath_or_buffer='pretreatmented_data.csv', header=None)\n",
    "FEATURES_INDEX = np.arange(50)\n",
    "LABELS_INDEX = 50\n",
    "BATCH_SIZE = 50\n",
    "LOSS_THRESHOLD = 1e-7\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spam_dataset = dataset[dataset[LABELS_INDEX] == 1]\n",
    "nonspam_dataset = dataset[dataset[LABELS_INDEX] == 0]\n",
    "nonspam_dataset.reset_index(drop=True, inplace=True)\n",
    "spam_dataset.reset_index(drop=True, inplace=True)\n",
    "nonspam_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rand_indxs_spam = np.arange(len(spam_dataset))\n",
    "rand_indxs_nonspam = np.arange(len(nonspam_dataset))\n",
    "np.random.shuffle(rand_indxs_nonspam)\n",
    "np.random.shuffle(rand_indxs_spam)\n",
    "\n",
    "spam_f_threshold = int(len(spam_dataset)*0.6)\n",
    "spam_s_threshold = int(len(spam_dataset)*0.9)\n",
    "\n",
    "nonspam_f_threshold = int(len(nonspam_dataset)*0.6)\n",
    "nonspam_s_threshold = int(len(nonspam_dataset)*0.9)\n",
    "\n",
    "learn_dataset = pd.concat([spam_dataset.iloc[rand_indxs_spam[:spam_f_threshold]],\n",
    "                          nonspam_dataset.iloc[rand_indxs_nonspam[:nonspam_f_threshold]]])\n",
    "\n",
    "valid_dataset = pd.concat([spam_dataset.iloc[rand_indxs_spam[spam_f_threshold:spam_s_threshold]],\n",
    "                          nonspam_dataset.iloc[rand_indxs_nonspam[nonspam_f_threshold:nonspam_s_threshold]]])\n",
    "\n",
    "test_dataset = pd.concat([spam_dataset.iloc[rand_indxs_spam[spam_s_threshold:]],\n",
    "                         nonspam_dataset.iloc[rand_indxs_nonspam[nonspam_s_threshold:]]])\n",
    "\n",
    "print(len(learn_dataset), len(valid_dataset), len(test_dataset), len(learn_dataset)+len(valid_dataset)+len(test_dataset), len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_INDEX = [0, 1]\n",
    "LABELS_INDEX = 2\n",
    "LOSS_THRESHOLD = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.980200</td>\n",
       "      <td>-0.735111</td>\n",
       "      <td>-0.573921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.758554</td>\n",
       "      <td>0.527610</td>\n",
       "      <td>2.068426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066156</td>\n",
       "      <td>0.404232</td>\n",
       "      <td>-0.447753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.919834</td>\n",
       "      <td>-0.836112</td>\n",
       "      <td>-0.615778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.676431</td>\n",
       "      <td>-0.458955</td>\n",
       "      <td>0.800171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  0.980200 -0.735111 -0.573921\n",
       "1 -1.758554  0.527610  2.068426\n",
       "2  0.066156  0.404232 -0.447753\n",
       "3  0.919834 -0.836112 -0.615778\n",
       "4 -0.676431 -0.458955  0.800171"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_dataset = pd.read_csv('../learn_data.csv', index_col=0)\n",
    "valid_dataset = pd.read_csv('../valid_data.csv', index_col=0)\n",
    "test_dataset = pd.read_csv('../test_data.csv', index_col=0)\n",
    "learn_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dataset = np.array(learn_dataset)\n",
    "valid_dataset = np.array(valid_dataset)\n",
    "test_dataset = np.array(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dataloader = DataLoader(learn_dataset, batch_size=len(learn_dataset), shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lin1): Linear(in_features=2, out_features=10, bias=True)\n",
       "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (lin3): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (lin4): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin1 = nn.Linear(2, 10)\n",
    "        self.lin2 = nn.Linear(10, 10)\n",
    "        self.lin3 = nn.Linear(10, 5)\n",
    "        self.lin4 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.lin1(x))\n",
    "        x = F.tanh(self.lin2(x))\n",
    "        x = F.tanh(self.lin3(x))\n",
    "        x = self.lin4(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1])\n",
      "tensor([ 0.1924,  0.3597, -0.2507, -0.7762, -0.0616, -0.4871,  0.1063,\n",
      "         0.4871,  0.2728,  0.0448])\n"
     ]
    }
   ],
   "source": [
    "#Инициализация весов\n",
    "STD = [(2/(2+10+1))**(1/2), (2/(10+10+1))**(1/2), (2/(10+5+1))**(1/2), (2/(5+1+1))**(1/2)]\n",
    "null_weigth = []\n",
    "STD.reverse()\n",
    "\n",
    "def get_weights(layer):\n",
    "    if (type(layer) == nn.Linear):\n",
    "        cur_std = STD.pop()\n",
    "        null_weigth.append(torch.randn(layer.weight.data.shape)*cur_std)\n",
    "        null_weigth.append(torch.randn(layer.bias.data.shape)*cur_std)\n",
    "\n",
    "net.apply(get_weights)\n",
    "for k in null_weigth:\n",
    "    print(k.shape)\n",
    "print(null_weigth[1])\n",
    "null_weigth.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1924,  0.3597, -0.2507, -0.7762, -0.0616, -0.4871,  0.1063,\n",
       "         0.4871,  0.2728,  0.0448])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_null_weigth = list(null_weigth)\n",
    "\n",
    "def init_weigths(layer):\n",
    "    if(type(layer) == nn.Linear):\n",
    "        layer.weight.data = (tmp_null_weigth.pop()).clone()\n",
    "        layer.bias.data = (tmp_null_weigth.pop()).clone()\n",
    "        \n",
    "net.apply(init_weigths)\n",
    "net.lin1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(size_average=True)\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learn_loss_by_lr = []\n",
    "valid_loss_by_lr = []\n",
    "test_loss_by_lr = []\n",
    "epoch_loss_by_lr = []\n",
    "mu_list = [0, 0.3, 0.6, 0.9]\n",
    "\n",
    "for mu in mu_list:\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=mu)\n",
    "    \n",
    "    tmp_null_weigth = list(null_weigth)\n",
    "    net.apply(init_weigths)\n",
    "    \n",
    "    learn_epoch_loss = []\n",
    "    valid_epoch_loss = []\n",
    "    test_epoch_loss = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for epoch in range(5*10**3):\n",
    "        loss_acc = []\n",
    "        for learn_data in learn_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            features, labels = learn_data[:, FEATURES_INDEX].float(), learn_data[:, LABELS_INDEX].float()\n",
    "            features.requres_grad = True\n",
    "            labels.requres_grad = True\n",
    "            outputs = net(features).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_acc.append(float(loss.data))\n",
    "        learn_epoch_loss.append(np.mean(loss_acc))\n",
    "        \n",
    "        #критерий останова\n",
    "        if (epoch > 10) and\\\n",
    "        (abs(learn_epoch_loss[len(learn_epoch_loss) - 1] - learn_epoch_loss[len(learn_epoch_loss) - 2]) < LOSS_THRESHOLD):\n",
    "            print(learn_epoch_loss[len(learn_epoch_loss) - 1] - learn_epoch_loss[len(learn_epoch_loss) - 2])\n",
    "            print('lr: ', lr, ' breaked on epoch: ', epoch, '\\n')\n",
    "            break\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            epoch_list.append(epoch)\n",
    "            for valid_data in valid_dataloader:\n",
    "                features, labels = valid_data[:, FEATURES_INDEX].float(), valid_data[:, LABELS_INDEX].float()\n",
    "                outputs = net(features).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_epoch_loss.append(float(loss.data))\n",
    "                \n",
    "            for test_data in valid_dataloader:\n",
    "                features, labels = test_data[:, FEATURES_INDEX].float(), test_data[:, LABELS_INDEX].float()\n",
    "                outputs = net(features).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_epoch_loss.append(float(loss.data))\n",
    "                \n",
    "    learn_loss_by_lr.append(learn_epoch_loss)\n",
    "    valid_loss_by_lr.append(valid_epoch_loss)\n",
    "    test_loss_by_lr.append(test_epoch_loss)\n",
    "    epoch_loss_by_lr.append(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn\n",
    "for num, i in enumerate(mu_list):\n",
    "    plt.plot(learn_loss_by_lr[num], label=str(i))\n",
    "plt.title('learn loss')\n",
    "plt.legend()\n",
    "plt.savefig('nikgdmoment/learn_loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "for num, i in enumerate(mu_list):\n",
    "    plt.plot(epoch_loss_by_lr[num], valid_loss_by_lr[num], label=str(i))\n",
    "plt.title('valid loss')\n",
    "plt.legend()\n",
    "plt.savefig('nikgdmoment/valid_loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "for num, i in enumerate(mu_list):\n",
    "    plt.plot(epoch_loss_by_lr[num], test_loss_by_lr[num], label=str(i))\n",
    "plt.title('test loss')\n",
    "plt.legend()\n",
    "plt.savefig('nikgdmoment/test_loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn\n",
    "for num, i in enumerate(mu_list):\n",
    "    print(learn_loss_by_lr[num][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "for num, i in enumerate(mu_list):\n",
    "    print(test_loss_by_lr[num][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learn_loss_by_lr = []\n",
    "valid_loss_by_lr = []\n",
    "test_loss_by_lr = []\n",
    "epoch_loss_by_lr = []\n",
    "mu_list = [1e-30, 0.3, 0.6, 0.9]\n",
    "\n",
    "for mu in mu_list:\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=mu, nesterov=True)\n",
    "    \n",
    "    tmp_null_weigth = list(null_weigth)\n",
    "    net.apply(init_weigths)\n",
    "    \n",
    "    learn_epoch_loss = []\n",
    "    valid_epoch_loss = []\n",
    "    test_epoch_loss = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for epoch in range(5*10**3):\n",
    "        loss_acc = []\n",
    "        for learn_data in learn_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            features, labels = learn_data[:, FEATURES_INDEX].float(), learn_data[:, LABELS_INDEX].float()\n",
    "            features.requres_grad = True\n",
    "            labels.requres_grad = True\n",
    "            outputs = net(features).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_acc.append(float(loss.data))\n",
    "        learn_epoch_loss.append(np.mean(loss_acc))\n",
    "        \n",
    "        #критерий останова\n",
    "        if (epoch > 10) and\\\n",
    "        (abs(learn_epoch_loss[len(learn_epoch_loss) - 1] - learn_epoch_loss[len(learn_epoch_loss) - 2]) < LOSS_THRESHOLD):\n",
    "            print(learn_epoch_loss[len(learn_epoch_loss) - 1] - learn_epoch_loss[len(learn_epoch_loss) - 2])\n",
    "            print('lr: ', lr, ' breaked on epoch: ', epoch, '\\n')\n",
    "            break\n",
    "            \n",
    "        if epoch % 1000 == 0:\n",
    "            epoch_list.append(epoch)\n",
    "            for valid_data in valid_dataloader:\n",
    "                features, labels = valid_data[:, FEATURES_INDEX].float(), valid_data[:, LABELS_INDEX].float()\n",
    "                outputs = net(features).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_epoch_loss.append(float(loss.data))\n",
    "                \n",
    "            for test_data in valid_dataloader:\n",
    "                features, labels = test_data[:, FEATURES_INDEX].float(), test_data[:, LABELS_INDEX].float()\n",
    "                outputs = net(features).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_epoch_loss.append(float(loss.data))\n",
    "                \n",
    "    learn_loss_by_lr.append(learn_epoch_loss)\n",
    "    valid_loss_by_lr.append(valid_epoch_loss)\n",
    "    test_loss_by_lr.append(test_epoch_loss)\n",
    "    epoch_loss_by_lr.append(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn\n",
    "for num, i in enumerate(mu_list):\n",
    "    plt.plot(learn_loss_by_lr[num], label=str(i))\n",
    "plt.title('learn loss')\n",
    "plt.legend()\n",
    "plt.savefig('nikgdmoment/learn_loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "for num, i in enumerate(mu_list):\n",
    "    plt.plot(epoch_loss_by_lr[num], valid_loss_by_lr[num], label=str(i))\n",
    "plt.title('valid loss')\n",
    "plt.legend()\n",
    "plt.savefig('nikgdmoment/valid_loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "for num, i in enumerate(mu_list):\n",
    "    plt.plot(epoch_loss_by_lr[num], test_loss_by_lr[num], label=str(i))\n",
    "plt.title('test loss')\n",
    "plt.legend()\n",
    "plt.savefig('nikgdmoment/test_loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn\n",
    "for num, i in enumerate(mu_list):\n",
    "    print(learn_loss_by_lr[num][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn\n",
    "for num, i in enumerate(mu_list):\n",
    "    print(test_loss_by_lr[num][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
