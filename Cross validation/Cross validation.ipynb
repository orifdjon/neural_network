{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3ed35dfa51c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import theano\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from neupy import algorithms, layers, init, environment, plots\n",
    "from neupy.exceptions import StopTraining\n",
    "from sklearn import datasets, cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=pd.read_csv(\"../all_data.csv\", index_col=0)\n",
    "all_data=np.array(all_data)\n",
    "eps=0.000001\n",
    "number_of_epochs=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpnet=algorithms.GradientDescent(\n",
    "     connection=[\n",
    "         layers.Input(2),\n",
    "         layers.Tanh(10, weight=init.XavierNormal(), bias=init.XavierNormal()),\n",
    "         layers.Tanh(10, weight=init.XavierNormal(), bias=init.XavierNormal()),\n",
    "         layers.Tanh(5, weight=init.XavierNormal(), bias=init.XavierNormal()),\n",
    "         layers.Linear(1, weight=init.XavierNormal(), bias=init.XavierNormal()),\n",
    "     ],\n",
    "     error='mse',\n",
    "     verbose=True,\n",
    "     shuffle_data=True,\n",
    "     show_epoch=50\n",
    "     \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "weight1=copy.deepcopy(bpnet.layers[1].weight.get_value())\n",
    "bias1=copy.deepcopy(bpnet.layers[1].bias.get_value())\n",
    "weight2=copy.deepcopy(bpnet.layers[2].weight.get_value())\n",
    "bias2=copy.deepcopy(bpnet.layers[2].bias.get_value())\n",
    "weight3=copy.deepcopy(bpnet.layers[3].weight.get_value())\n",
    "bias3=copy.deepcopy(bpnet.layers[3].bias.get_value())\n",
    "weight4=copy.deepcopy(bpnet.layers[4].weight.get_value())\n",
    "bias4=copy.deepcopy(bpnet.layers[4].bias.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors=[]\n",
    "def signal(net):\n",
    "    test_errors.append(((net.predict(test_params)-test_target)**2).sum()/len(test_target))\n",
    "    if len(net.errors)>20:\n",
    "        if abs(net.errors[len(net.errors)-1]-net.errors[len(net.errors)-2]) < eps:\n",
    "            raise StopTraining(\"Training has been interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_error_task=[]\n",
    "test_error_task=[]\n",
    "for i in range(10):\n",
    "    test_errors=[]\n",
    "    np.random.seed(i)\n",
    "    np.random.shuffle(all_data)\n",
    "    learn_data = all_data[0: int(0.6*len(all_data))]\n",
    "    validation_data = all_data[int(0.6*len(all_data)): int(0.9*len(all_data))]\n",
    "    test_data = all_data[int(0.9*len(all_data)): len(all_data)]\n",
    "    learn_params=learn_data[:,[0,1]]\n",
    "    learn_target=learn_data[:,[2]]\n",
    "    test_params=test_data[:,[0,1]]\n",
    "    test_target=test_data[:,[2]]\n",
    "    validation_params=validation_data[:,[0,1]]\n",
    "    validation_target=validation_data[:,[2]]\n",
    "    bpnet=algorithms.GradientDescent(\n",
    "     connection=[\n",
    "         layers.Input(2),\n",
    "         layers.Tanh(10, weight=copy.deepcopy(weight1), bias=copy.deepcopy(bias1)),\n",
    "         layers.Tanh(10, weight=copy.deepcopy(weight2), bias=copy.deepcopy(bias2)),\n",
    "         layers.Tanh(5, weight=copy.deepcopy(weight3), bias=copy.deepcopy(bias3)),\n",
    "         layers.Linear(1, weight=copy.deepcopy(weight4), bias=copy.deepcopy(bias4)),\n",
    "     ],\n",
    "     error='mse',\n",
    "     step=0.05,\n",
    "     epoch_end_signal=signal,\n",
    "     verbose=True,\n",
    "     shuffle_data=True,\n",
    "     show_epoch=50 \n",
    "    )\n",
    "    bpnet.train(learn_params, learn_target, validation_params, validation_target, epochs=number_of_epochs)\n",
    "    learn_error_task.append(bpnet.errors.last())\n",
    "    test_error_task.append(test_errors[len(test_errors)-1])\n",
    "print(\"Monte karlo learn error mean \"+str(np.array(learn_error_task).mean()))\n",
    "print(\"Monte karlo learn error std \"+str(np.array(learn_error_task).std()))\n",
    "print(\"Monte karlo test error mean \"+str(np.array(test_error_task).mean()))\n",
    "print(\"Monte karlo test error std \"+str(np.array(test_error_task).std()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors=[]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(all_data)\n",
    "learn_data = all_data[0: int(0.6*len(all_data))]\n",
    "validation_data = all_data[int(0.6*len(all_data)): int(0.9*len(all_data))]\n",
    "test_data = all_data[int(0.9*len(all_data)): len(all_data)]\n",
    "learn_params=learn_data[:,[0,1]]\n",
    "learn_target=learn_data[:,[2]]\n",
    "test_params=test_data[:,[0,1]]\n",
    "test_target=test_data[:,[2]]\n",
    "validation_params=validation_data[:,[0,1]]\n",
    "validation_target=validation_data[:,[2]]\n",
    "bpnet=algorithms.GradientDescent(\n",
    "    connection=[\n",
    "     layers.Input(2),\n",
    "     layers.Tanh(10, weight=copy.deepcopy(weight1), bias=copy.deepcopy(bias1)),\n",
    "     layers.Tanh(10, weight=copy.deepcopy(weight2), bias=copy.deepcopy(bias2)),\n",
    "     layers.Tanh(5, weight=copy.deepcopy(weight3), bias=copy.deepcopy(bias3)),\n",
    "     layers.Linear(1, weight=copy.deepcopy(weight4), bias=copy.deepcopy(bias4)),\n",
    "    ],\n",
    "    error='mse',\n",
    "    step=0.05,\n",
    "    epoch_end_signal=signal,\n",
    "    verbose=True,\n",
    "    shuffle_data=True,\n",
    "    show_epoch=50 \n",
    ")\n",
    "bpnet.train(learn_params, learn_target, validation_params, validation_target, epochs=number_of_epochs)\n",
    "print(\"Holdout learn error \"+str(bpnet.errors.last()))\n",
    "print(\"Holdout test error \"+str(test_errors[len(test_errors)-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(57)\n",
    "np.random.shuffle(all_data)\n",
    "learn_error_task=[]\n",
    "test_error_task=[]\n",
    "for train_indices, test_indices in cross_validation.KFold(len(all_data), n_folds=10, shuffle=True):\n",
    "    test_errors=[]\n",
    "    tmp_data=all_data[train_indices]\n",
    "    test_data=all_data[test_indices]\n",
    "    learn_data=tmp_data[0:int(0.7*len(tmp_data))]\n",
    "    validation_data=tmp_data[int(0.7*len(tmp_data)):len(tmp_data)]\n",
    "    learn_params=learn_data[:,[0,1]]\n",
    "    learn_target=learn_data[:,[2]]\n",
    "    test_params=test_data[:,[0,1]]\n",
    "    test_target=test_data[:,[2]]\n",
    "    validation_params=validation_data[:,[0,1]]\n",
    "    validation_target=validation_data[:,[2]]\n",
    "    bpnet=algorithms.GradientDescent(\n",
    "        connection=[\n",
    "         layers.Input(2),\n",
    "         layers.Tanh(10, weight=copy.deepcopy(weight1), bias=copy.deepcopy(bias1)),\n",
    "         layers.Tanh(10, weight=copy.deepcopy(weight2), bias=copy.deepcopy(bias2)),\n",
    "         layers.Tanh(5, weight=copy.deepcopy(weight3), bias=copy.deepcopy(bias3)),\n",
    "         layers.Linear(1, weight=copy.deepcopy(weight4), bias=copy.deepcopy(bias4)),\n",
    "        ],\n",
    "        error='mse',\n",
    "        step=0.05,\n",
    "        epoch_end_signal=signal,\n",
    "        verbose=True,\n",
    "        shuffle_data=True,\n",
    "        show_epoch=50 \n",
    "    )\n",
    "    bpnet.train(learn_params, learn_target, validation_params, validation_target, epochs=number_of_epochs)\n",
    "    learn_error_task.append(bpnet.errors.last())\n",
    "    test_error_task.append(test_errors[len(test_errors)-1])\n",
    "print(\"10 fold learn error mean \"+str(np.array(learn_error_task).mean()))\n",
    "print(\"10 fold learn error std \"+str(np.array(learn_error_task).std()))\n",
    "print(\"10 fold test error mean \"+str(np.array(test_error_task).mean()))\n",
    "print(\"10 fold test error std \"+str(np.array(test_error_task).std()))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_error_mean=[]\n",
    "learn_error_std=[]\n",
    "test_error_mean=[]\n",
    "test_error_std=[]\n",
    "fold_list=[2, 3, 4, 5, 7, 10, 13, 15, 20, 25]\n",
    "for K in fold_list:\n",
    "    np.random.shuffle(all_data)\n",
    "    learn_error_task=[]\n",
    "    test_error_task=[]\n",
    "    for train_indices, test_indices in cross_validation.KFold(len(all_data), n_folds=K, shuffle=True):\n",
    "        test_errors=[]\n",
    "        tmp_data=all_data[train_indices]\n",
    "        test_data=all_data[test_indices]\n",
    "        learn_data=tmp_data[0:int(0.7*len(tmp_data))]\n",
    "        validation_data=tmp_data[int(0.7*len(tmp_data)):len(tmp_data)]\n",
    "        learn_params=learn_data[:,[0,1]]\n",
    "        learn_target=learn_data[:,[2]]\n",
    "        test_params=test_data[:,[0,1]]\n",
    "        test_target=test_data[:,[2]]\n",
    "        validation_params=validation_data[:,[0,1]]\n",
    "        validation_target=validation_data[:,[2]]\n",
    "        bpnet=algorithms.GradientDescent(\n",
    "            connection=[\n",
    "             layers.Input(2),\n",
    "             layers.Tanh(10, weight=copy.deepcopy(weight1), bias=copy.deepcopy(bias1)),\n",
    "             layers.Tanh(10, weight=copy.deepcopy(weight2), bias=copy.deepcopy(bias2)),\n",
    "             layers.Tanh(5, weight=copy.deepcopy(weight3), bias=copy.deepcopy(bias3)),\n",
    "             layers.Linear(1, weight=copy.deepcopy(weight4), bias=copy.deepcopy(bias4)),\n",
    "            ],\n",
    "            error='mse',\n",
    "            step=0.05,\n",
    "            epoch_end_signal=signal,\n",
    "            verbose=True,\n",
    "            shuffle_data=True,\n",
    "            show_epoch=50 \n",
    "        )\n",
    "        bpnet.train(learn_params, learn_target, validation_params, validation_target, epochs=number_of_epochs)\n",
    "        learn_error_task.append(bpnet.errors.last())\n",
    "        test_error_task.append(test_errors[len(test_errors)-1])\n",
    "    learn_error_mean.append(np.array(learn_error_task).mean())\n",
    "    learn_error_std.append(np.array(learn_error_task).std())\n",
    "    test_error_mean.append(np.array(test_error_task).mean())\n",
    "    test_error_std.append(np.array(test_error_task).std())  \n",
    "plt.clf()\n",
    "plt.grid(True)\n",
    "plt.plot(fold_list, learn_error_mean, color='r', label='train mean') \n",
    "plt.plot(fold_list,learn_error_std, color='b', label='train std') \n",
    "plt.plot(fold_list, test_error_mean, color='g', label='test mean')\n",
    "plt.plot(fold_list, test_error_std, color='k', label='test std')\n",
    "plt.legend()\n",
    "plt.savefig('k_fold_test.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(87)\n",
    "np.random.shuffle(all_data)\n",
    "learn_error_task=[]\n",
    "test_error_task=[]\n",
    "for train_indices, test_indices in cross_validation.LeaveOneOut(len(all_data)):\n",
    "    test_errors=[]\n",
    "    tmp_data=all_data[train_indices]\n",
    "    test_data=all_data[test_indices]\n",
    "    learn_data=tmp_data[0:int(0.7*len(tmp_data))]\n",
    "    validation_data=tmp_data[int(0.7*len(tmp_data)):len(tmp_data)]\n",
    "    learn_params=learn_data[:,[0,1]]\n",
    "    learn_target=learn_data[:,[2]]\n",
    "    test_params=test_data[:,[0,1]]\n",
    "    test_target=test_data[:,[2]]\n",
    "    validation_params=validation_data[:,[0,1]]\n",
    "    validation_target=validation_data[:,[2]]\n",
    "    bpnet=algorithms.GradientDescent(\n",
    "        connection=[\n",
    "         layers.Input(2),\n",
    "         layers.Tanh(10, weight=copy.deepcopy(weight1), bias=copy.deepcopy(bias1)),\n",
    "         layers.Tanh(10, weight=copy.deepcopy(weight2), bias=copy.deepcopy(bias2)),\n",
    "         layers.Tanh(5, weight=copy.deepcopy(weight3), bias=copy.deepcopy(bias3)),\n",
    "         layers.Linear(1, weight=copy.deepcopy(weight4), bias=copy.deepcopy(bias4)),\n",
    "        ],\n",
    "        error='mse',\n",
    "        step=0.05,\n",
    "        epoch_end_signal=signal,\n",
    "        verbose=True,\n",
    "        shuffle_data=True,\n",
    "        show_epoch=50 \n",
    "    )\n",
    "    bpnet.train(learn_params, learn_target, validation_params, validation_target, epochs=number_of_epochs)\n",
    "    learn_error_task.append(bpnet.errors.last())\n",
    "    test_error_task.append(test_errors[len(test_errors)-1])\n",
    "print(\"Loocv learn error mean \"+str(np.array(learn_error_task).mean()))\n",
    "print(\"Loocv learn error std \"+str(np.array(learn_error_task).std()))\n",
    "print(\"Loocv test error mean \"+str(np.array(test_error_task).mean()))\n",
    "print(\"Loocv test error std \"+str(np.array(test_error_task).std()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
